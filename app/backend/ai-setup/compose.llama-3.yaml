version: '3.9'

services:
  llama3:
    image: ollama/ollama
    container_name: llama-3-2
    restart: unless-stopped
    environment:
      - OLLAMA_KEEP_ALIVE=-1
      #- OLLAMA_NUM_PARALLEL=4 for several request at the same time
    ports:
      - 11435:11434  # Map host 11435 to container 11434
    volumes:
      - ./ollama_data:/root/.ollama
    entrypoint: ["/bin/bash", "-c", "\
      ollama serve & \
      sleep 5 && \
      ollama pull llama3.2:3b && \
      wait"]

# docker compose -f compose.llama-3.yaml up